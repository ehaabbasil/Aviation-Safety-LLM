{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJbNLuuwm0an"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJsTyU4wU5Jt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df = pd.read_csv('/content/drive/MyDrive/Aviation-LLM/ASRS_DBOnline_DS1.csv', header=[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRV4D1d0dk6j"
      },
      "outputs": [],
      "source": [
        "# Flatten the columns for easier access\n",
        "df.columns = ['_'.join(col).strip() for col in df.columns.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyATH6l5drsz"
      },
      "outputs": [],
      "source": [
        "keep_columns = [\n",
        "    '_ACN',\n",
        "    'Time_Date',\n",
        "    'Time_Local Time Of Day',\n",
        "    'Place_State Reference',\n",
        "    'Aircraft 1_Aircraft Operator',\n",
        "    'Aircraft 1_Mission',\n",
        "    'Aircraft 1_Make Model Name',\n",
        "    'Aircraft 1_Flight Phase',\n",
        "    'Component_Aircraft Component',\n",
        "    'Person 1_Function',\n",
        "    'Person 1_Qualification',\n",
        "    'Person 1_Human Factors',\n",
        "    'Events_Anomaly',\n",
        "    'Events_Detector',\n",
        "    'Events_When Detected',\n",
        "    'Events_Result',\n",
        "    'Assessments_Contributing Factors / Situations',\n",
        "    'Assessments_Primary Problem',\n",
        "    'Report 1_Narrative',\n",
        "    'Report 1_Synopsis'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxoHVF_AfO4O"
      },
      "outputs": [],
      "source": [
        "# Select only the specified columns\n",
        "df = df[keep_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07to06pAq7OR"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing narratives or synopses, as they are essential\n",
        "df = df.dropna(subset=['Report 1_Narrative', 'Report 1_Synopsis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg6a0G4Kq_zB"
      },
      "outputs": [],
      "source": [
        "# Clean text by removing extra spaces\n",
        "df['Report 1_Narrative'] = df['Report 1_Narrative'].str.strip()\n",
        "df['Report 1_Synopsis'] = df['Report 1_Synopsis'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlIS_rTTrF4T"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned data for future use\n",
        "df.to_csv('cleaned_ASRS_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw2mXxxQra7c"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# files.download('cleaned_ASRS_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKOPrzuriLH8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdrHJsgEmJKO"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZOza_kgyJJz"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your DataFrame loaded from 'cleaned_ASRS_data.csv'\n",
        "\n",
        "total_incidents = len(df)\n",
        "print(f\"Total number of incidents: {total_incidents}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhcoAy4CmI2K"
      },
      "source": [
        "# **EDA **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uym-oynkmdD-"
      },
      "outputs": [],
      "source": [
        "# Convert Time_Date to datetime\n",
        "df['Time_Date'] = pd.to_datetime(df['Time_Date'], format='%Y%m')\n",
        "\n",
        "# Extract year and month for grouping\n",
        "df['Year'] = df['Time_Date'].dt.year\n",
        "df['Month'] = df['Time_Date'].dt.to_period('M')\n",
        "\n",
        "# Verify the conversion\n",
        "print(df[['Time_Date', 'Year', 'Month']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKAE4tCgnbrS"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtlU6a8sEiU8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "flight_phase_counts = df['Aircraft 1_Flight Phase'].value_counts()\n",
        "\n",
        "print(\"Flight Phase Counts:\")\n",
        "print(flight_phase_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoJLg30dns0_"
      },
      "outputs": [],
      "source": [
        "# Flight phase distribution\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the 'Aircraft 1_Flight Phase' column\n",
        "\n",
        "# Get the top N most popular flight phases (e.g., top 10)\n",
        "top_n = 5  # You can change this to the desired number\n",
        "top_flight_phases = df['Aircraft 1_Flight Phase'].value_counts().nlargest(top_n).index\n",
        "\n",
        "# Filter the DataFrame to include only the top flight phases\n",
        "filtered_df = df[df['Aircraft 1_Flight Phase'].isin(top_flight_phases)]\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = filtered_df['Aircraft 1_Flight Phase'].value_counts().plot(kind='bar')\n",
        "plt.title(f'Distribution of Incidents by Top {top_n} Flight Phases')\n",
        "plt.xlabel('Flight Phase')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Add the numbers on top of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "\n",
        "plt.savefig('flight_phase_distribution.png')  # Save the plot to a file\n",
        "plt.show()                                   # Display the plot\n",
        "# files.download('flight_phase_distribution.png') # Download the saved file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tERwu-ukcbab"
      },
      "outputs": [],
      "source": [
        "pip install -U kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAC5Qyko5wir"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'svg' # or 'png', 'jpg', etc.\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the 'Assessments_Primary Problem' column\n",
        "\n",
        "# Get the value counts for the primary problems\n",
        "primary_problem_counts = df['Assessments_Primary Problem'].value_counts()\n",
        "\n",
        "# Calculate total incidents for Primary Problem Distribution\n",
        "total_incidents_primary_problem = primary_problem_counts.sum()\n",
        "\n",
        "# Create a Plotly table\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(values=['Primary Problem', 'Count'],\n",
        "                fill_color='paleturquoise',\n",
        "                align='left'),\n",
        "    cells=dict(values=[primary_problem_counts.index, primary_problem_counts.values],\n",
        "               fill_color='lavender',\n",
        "               align='left'))\n",
        "])\n",
        "\n",
        "pio.write_image(fig, 'primary_problem_distribution.png') # Save the chart as PNG\n",
        "fig.show()\n",
        "\n",
        "print(f\"Total number of incidents in Primary Problem Distribution: {total_incidents_primary_problem}\")\n",
        "# files.download('primary_problem_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oUoP6mKo2BJ"
      },
      "outputs": [],
      "source": [
        "# Primary problem distribution\n",
        "plt.figure(figsize=(8,8))\n",
        "df['Assessments_Primary Problem'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "plt.title('Distribution of Primary Problems')\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('primary_problem_distribution_graph.png')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('primary_problem_distribution_graph.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln7-5opOyRVc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming 'df' is your DataFrame containing the 'Year' column\n",
        "\n",
        "incidents_by_year = df.groupby('Year')['Year'].count()\n",
        "\n",
        "print(\"Total Number of Incidents by Year:\")\n",
        "print(incidents_by_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3X-pvcBpqkQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of Incidents per Year\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = df['Year'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Number of Incidents per Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Incidents')\n",
        "\n",
        "# Add count labels on top of bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('incidents_per_year.png')  # You can change the filename and format if needed\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('incidents_per_year.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sCyZVKCGiwQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Group by year and get the value counts of primary problems for each year\n",
        "year_primary_problem = df.groupby('Year')['Assessments_Primary Problem'].value_counts().reset_index(name='Count')\n",
        "\n",
        "# Print the total number of incidents for each year\n",
        "print(\"Total Number of Incidents by Year:\")\n",
        "incidents_by_year = df.groupby('Year')['Year'].count()  # Calculate incidents_by_year\n",
        "print(incidents_by_year)  # Print incidents_by_year\n",
        "\n",
        "# Print the year and the primary problem for each year\n",
        "print(\"\\nYear and Primary Problem:\")\n",
        "for index, row in year_primary_problem.iterrows():\n",
        "    year = row['Year']\n",
        "    primary_problem = row['Assessments_Primary Problem']\n",
        "    count = row['Count']\n",
        "    print(f\"Year: {year}, Primary Problem: {primary_problem}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-2wD-eG66yL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "ct = pd.crosstab(df['Year'], df['Assessments_Primary Problem'])\n",
        "ct.plot(kind='bar', stacked=True, figsize=(12,6))\n",
        "plt.title('Distribution of Primary Problems Over Years')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Incidents')\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('primary_problems_over_years.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('primary_problems_over_years.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZqSLZKxrxau"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "print(df['Time_Local Time Of Day'].unique())\n",
        "plt.figure(figsize=(10,6))\n",
        "df['Time_Local Time Of Day'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of Incidents by Local Time of Day')\n",
        "plt.xlabel('Time of Day')\n",
        "plt.ylabel('Number of Incidents')\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('incidents_by_time_of_day.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('incidents_by_time_of_day.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtCK5AqI0i-E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "\n",
        "aircraft_incident_counts = df['Aircraft 1_Make Model Name'].value_counts()\n",
        "\n",
        "print(\"Aircraft Name and Number of Incidents:\")\n",
        "print(aircraft_incident_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_k_aujPqCd7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Which Aircraft had most incidents?\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the 'Aircraft 1_Make Model Name' column\n",
        "\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size for better readability\n",
        "ax = df['Aircraft 1_Make Model Name'].value_counts().head(10).plot(kind='barh')  # Use horizontal bar chart\n",
        "plt.title('Top 10 Aircraft Models by Number of Incidents')\n",
        "plt.xlabel('Number of Incidents')\n",
        "plt.ylabel('Aircraft Model')\n",
        "\n",
        "# Add count labels to the right of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('top_aircraft_models.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('top_aircraft_models.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxETIn0PI5Qj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "top_components_counts = df['Component_Aircraft Component'].value_counts().head(10)\n",
        "\n",
        "print(\"Top 10 Aircraft Components and Their Counts:\")\n",
        "print(top_components_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYkovQ_utq7m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = df['Component_Aircraft Component'].value_counts().head(10).plot(kind='barh')  # Use horizontal bar chart\n",
        "plt.title('Top 10 Aircraft Components Involved in Incidents')\n",
        "plt.xlabel('Number of Incidents')\n",
        "plt.ylabel('Aircraft Component')\n",
        "\n",
        "# Add count labels to the right of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('top_aircraft_components.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('top_aircraft_components.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXVcQkiV8X1G"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "top_components = df['Component_Aircraft Component'].value_counts().head(5).index\n",
        "top_phases = df['Aircraft 1_Flight Phase'].value_counts().head(5).index\n",
        "filtered_df = df[df['Component_Aircraft Component'].isin(top_components) & df['Aircraft 1_Flight Phase'].isin(top_phases)]\n",
        "ct = pd.crosstab(filtered_df['Component_Aircraft Component'], filtered_df['Aircraft 1_Flight Phase'])\n",
        "\n",
        "# Create the figure and axes before plotting the heatmap\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "ax = sns.heatmap(ct, annot=True, cmap='Blues', fmt='d')  # Assign heatmap to ax\n",
        "\n",
        "# Set title and adjust layout using the axes object\n",
        "ax.set_title('Heatmap of Top 5 Components vs Top 5 Flight Phases')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.subplots_adjust(bottom=0.25)\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('heatmap_components_phases.png', bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('heatmap_components_phases.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRb9NsTauKMQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = df['Person 1_Human Factors'].value_counts().head(10).plot(kind='barh')  # Use horizontal bar chart\n",
        "plt.title('Top 10 Human Factors in Incidents')\n",
        "plt.xlabel('Number of Incidents')\n",
        "plt.ylabel('Human Factor')\n",
        "\n",
        "# Add count labels to the right of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('top_human_factors.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('top_human_factors.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob8PpJTDveHm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the 'Events_Detector' column\n",
        "\n",
        "# Calculate value counts and filter for values greater than 1%\n",
        "detector_counts = df['Events_Detector'].value_counts(normalize=True)  # Normalize to get percentages\n",
        "filtered_counts = detector_counts[detector_counts > 0.01]  # Filter for values > 1%\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(filtered_counts,\n",
        "        labels=None,  # Remove labels on the pie chart\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        textprops={'fontsize': 10},\n",
        "        pctdistance=0.85)\n",
        "plt.title('Distribution of Detectors for Anomalies (Above 1%)', fontsize=12)\n",
        "plt.legend(filtered_counts.index, loc='best', bbox_to_anchor=(1, 0.5))  # Place legend outside\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('anomaly_detectors_distribution.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('anomaly_detectors_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gglBMwkl0DVC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Import numpy\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the 'Events_Result' column\n",
        "\n",
        "# Get the top N most frequent event results\n",
        "top_n = 10  # Adjust as needed\n",
        "event_result_counts = df['Events_Result'].value_counts().head(top_n)\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({'Event Result': event_result_counts.index, 'Count': event_result_counts.values})\n",
        "\n",
        "# Create a list of unique colors for each bar\n",
        "colors = plt.cm.get_cmap('tab10', top_n)  # Use a colormap with enough colors\n",
        "color_list = [colors(i) for i in range(top_n)]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(np.arange(len(plot_df)), plot_df['Count'], color=color_list)  # Use color_list for bar colors\n",
        "plt.title('Top 10 Outcomes of Events')\n",
        "plt.xlabel('Event Result Index')  # Use index instead of full description\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(np.arange(len(plot_df)), np.arange(len(plot_df)))  # Set x-axis ticks to indices\n",
        "\n",
        "# Create a custom legend\n",
        "plt.legend(bars, plot_df['Event Result'], title='Event Result', loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "# Add count labels on top of bars with adjusted alignment\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.05 * height,  # Adjust vertical position\n",
        "             str(int(height)), ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.ylim(0, max(plot_df['Count']) * 1.1)  # Adjust y-axis limit\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('top_10_outcomes.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('top_10_outcomes.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdFkcTG6ufyF"
      },
      "outputs": [],
      "source": [
        " #Assuming df is your DataFrame loaded from 'cleaned_ASRS_data.csv'\n",
        "# Split anomalies by semicolon and explode into individual entries\n",
        "anomalies_split = df['Events_Anomaly'].str.split(';').explode().str.strip()\n",
        "\n",
        "# Remove 'Anomaly.' prefix if present to shorten labels\n",
        "anomalies_split = anomalies_split.str.replace('Anomaly.', '', regex=False)\n",
        "\n",
        "# Get the top 10 most common anomalies\n",
        "top_anomalies = anomalies_split.value_counts().head(10)\n",
        "\n",
        "# Assuming df is your DataFrame loaded from 'cleaned_ASRS_data.csv'\n",
        "# Split anomalies by semicolon and explode into individual entries\n",
        "anomalies_split = df['Events_Anomaly'].str.split(';').explode().str.strip()\n",
        "\n",
        "# Remove 'Anomaly.' prefix if present to shorten labels\n",
        "anomalies_split = anomalies_split.str.replace('Anomaly.', '', regex=False)\n",
        "\n",
        "# Get the top 10 most common anomalies\n",
        "top_anomalies = anomalies_split.value_counts().head(10)\n",
        "\n",
        "\n",
        "# Create a horizontal bar chart with adjusted settings and colors\n",
        "plt.figure(figsize=(12, 6))\n",
        "num_bars = len(top_anomalies)  # Define num_bars here\n",
        "colors = plt.cm.get_cmap('viridis', num_bars)  # Choose a colormap\n",
        "ax = top_anomalies.plot(kind='barh', color=[colors(i) for i in range(num_bars)])  # Apply colors\n",
        "\n",
        "plt.title('Top 10 Most Common Individual Anomalies')\n",
        "plt.xlabel('Number of Incidents')  # Swapped x and y labels\n",
        "plt.ylabel('Anomaly Type')  # Swapped x and y labels\n",
        "\n",
        "# Add count labels to the right of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent clipping\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('anomaly_chart.png')  # Save the plot\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('anomaly_chart.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fpmp-ei3RwU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files  # Import files for download\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "# Split and explode the 'Events_Result' column\n",
        "results_split = df['Events_Result'].str.split(';').explode().str.strip()\n",
        "\n",
        "# Calculate frequencies\n",
        "result_counts = results_split.value_counts()\n",
        "\n",
        "# Plot top 10 individual outcomes\n",
        "top_results = result_counts.head(10)  # Define top_results here\n",
        "\n",
        "# Plot top 10 individual outcomes with count labels and color\n",
        "plt.figure(figsize=(12, 6))\n",
        "num_bars = len(top_results)\n",
        "colors = plt.cm.get_cmap('viridis', num_bars)  # Choose a colormap\n",
        "ax = top_results.plot(kind='barh', color=[colors(i) for i in range(num_bars)])  # Apply colors\n",
        "plt.title('Top 10 Most Common Individual Event Results')\n",
        "plt.xlabel('Number of Incidents')\n",
        "plt.ylabel('Event Result')\n",
        "\n",
        "# Add count labels to the right of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_outcomes_chart.png')\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('top_outcomes_chart.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1t_YS7X3Is9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Split and explode the 'Events_Result' column\n",
        "results_split = df['Events_Result'].str.split(';').explode().str.strip()\n",
        "\n",
        "# Calculate frequencies\n",
        "result_counts = results_split.value_counts()\n",
        "\n",
        "# Plot top 10 individual outcomes\n",
        "top_results = result_counts.head(10)\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size if needed\n",
        "top_results.plot(kind='barh')  # Change to horizontal bar chart\n",
        "plt.title('Top 10 Most Common Individual Event Results')\n",
        "plt.xlabel('Number of Incidents')  # Swap x and y labels\n",
        "plt.ylabel('Event Result')  # Swap x and y labels\n",
        "plt.yticks(fontsize=10)  # Adjust y-axis tick font size\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_outcomes_chart.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njt7tloH4ZXj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "severe_keywords = ['injury', 'incapacitation', 'evacuated', 'damage', 'emergency']\n",
        "\n",
        "# Calculate severe_counts\n",
        "severe_counts = df['Events_Result'].str.contains('|'.join(severe_keywords), case=False).sum()\n",
        "# Create a Series for plotting\n",
        "severe_counts_series = pd.Series(severe_counts, index=['Severe Outcomes'], name='Count')\n",
        "\n",
        "\n",
        "# Plot severe outcomes using a horizontal bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = severe_counts_series.plot(kind='barh')  # Change to horizontal bar chart (barh)\n",
        "plt.title('Frequency of Severe Event Results')\n",
        "plt.xlabel('Number of Incidents')  # Swap x and y labels\n",
        "plt.ylabel('Event Result')  # Swap x and y labels\n",
        "\n",
        "# Add count labels to the right of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(p.get_width()), (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.savefig('severe_outcomes_chart.png')  # Save the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7JW5IVB5Y1w"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWWT-TyP5kT5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL_IGzDk5kqh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'result_counts' is your Series containing the frequency of each event result\n",
        "\n",
        "# Define a threshold for rarity, e.g., outcomes with fewer than 5 incidents\n",
        "rare_threshold = 5\n",
        "rare_results = result_counts[result_counts < rare_threshold]\n",
        "\n",
        "# Create a DataFrame for the table\n",
        "rare_outcomes_table = pd.DataFrame({'Event Result': rare_results.index, 'Count': rare_results.values})\n",
        "\n",
        "# Display the table\n",
        "print(\"Rare Outcomes (fewer than 5 incidents):\")\n",
        "print(rare_outcomes_table.to_string(index=False))  # Display table without index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3204QOLE5_xc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# ... (your existing code for defining rare_threshold and filtering rare_results)\n",
        "\n",
        "# Create a horizontal bar chart for rare outcomes\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "rare_outcomes_table.plot(x='Event Result', y='Count', kind='barh', legend=False)\n",
        "plt.title('Rare Outcomes (Fewer than 5 Incidents)')\n",
        "plt.xlabel('Number of Incidents')\n",
        "plt.ylabel('Event Result')\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('rare_outcomes_chart.png')  # Choose a filename\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Download the saved file\n",
        "# files.download('rare_outcomes_chart.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDphRZKiW0M"
      },
      "source": [
        "Verify and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay0qG1zdiavV"
      },
      "outputs": [],
      "source": [
        "print(f\"Total rows: {len(df)}\")\n",
        "print(df[['Report 1_Narrative', 'Report 1_Synopsis']].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XldC9rhdcdOI"
      },
      "outputs": [],
      "source": [
        "# Check for null values in narratives and synopses\n",
        "null_narratives = df['Report 1_Narrative'].isnull().sum()\n",
        "null_synopses = df['Report 1_Synopsis'].isnull().sum()\n",
        "\n",
        "print(f\"Number of null narratives: {null_narratives}\")\n",
        "print(f\"Number of null synopses: {null_synopses}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r4jer83iLmL"
      },
      "outputs": [],
      "source": [
        "# Summarize dataset\n",
        "total_rows = len(df)\n",
        "print(f\"Total number of incidents: {total_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym35LUzNiTlC"
      },
      "outputs": [],
      "source": [
        "# Check text lengths\n",
        "df['narrative_length'] = df['Report 1_Narrative'].apply(lambda x: len(str(x).split()))\n",
        "df['synopsis_length'] = df['Report 1_Synopsis'].apply(lambda x: len(str(x).split()))\n",
        "print(f\"Max narrative length (words): {df['narrative_length'].max()}\")\n",
        "print(f\"Average narrative length (words): {df['narrative_length'].mean():.2f}\")\n",
        "print(f\"Max synopsis length (words): {df['synopsis_length'].max()}\")\n",
        "print(f\"Average synopsis length (words): {df['synopsis_length'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29EAe0pRQJ0L"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVOpLyDpQPiu"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ueQ-7plMBz"
      },
      "source": [
        "Fine Tuning using BART"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqhOktPvUa_"
      },
      "source": [
        "Tokenize and Prepare the Dataset for Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "import numpy as np\n",
        "import random\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "# Load the cleaned dataset\n",
        "dataset = load_dataset('csv', data_files='cleaned_ASRS_data.csv')\n",
        "\n",
        "# Split dataset: 886 train, 110 val, 111 test with a fixed seed\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1, seed=random_seed)  # 111 test\n",
        "train_val = dataset['train'].train_test_split(test_size=0.1111, seed=random_seed)  # 886 train, 110 val\n",
        "train_dataset = train_val['train']\n",
        "val_dataset = train_val['test']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Save the test dataset to a CSV file to ensure consistency\n",
        "test_df = pd.DataFrame(test_dataset)\n",
        "test_df.to_csv('test_split.csv', index=False)\n",
        "print(f\"Test dataset saved to 'test_split.csv' with {len(test_df)} samples\")"
      ],
      "metadata": {
        "id": "pg2fk6DAhbpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbHKoT6VMDuh"
      },
      "outputs": [],
      "source": [
        "# # Load dataset\n",
        "# df = pd.read_csv('cleaned_ASRS_data.csv')\n",
        "\n",
        "# Text Normalization\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9.,!? ]', '', text)\n",
        "    return text\n",
        "\n",
        "df['Report 1_Narrative'] = df['Report 1_Narrative'].apply(normalize_text)\n",
        "df['Report 1_Synopsis'] = df['Report 1_Synopsis'].apply(normalize_text)\n",
        "\n",
        "# Calculate narrative tokens\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "df['narrative_tokens'] = df['Report 1_Narrative'].apply(\n",
        "    lambda x: len(tokenizer.encode(x, add_special_tokens=True))\n",
        ")\n",
        "\n",
        "# Handle rare flight phases\n",
        "def get_primary_phase(phase_str):\n",
        "    return phase_str.split(';')[0].strip()\n",
        "\n",
        "df['primary_flight_phase'] = df['Aircraft 1_Flight Phase'].apply(get_primary_phase)\n",
        "\n",
        "# Remove rare classes with fewer than 2 samples\n",
        "phase_counts = df['primary_flight_phase'].value_counts()\n",
        "rare_phases = phase_counts[phase_counts < 2].index\n",
        "df = df[~df['primary_flight_phase'].isin(rare_phases)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffLgZcBEOSa6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uodAiyGtMLTP"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy='steps',  # Updated parameter name\n",
        "    eval_steps=110,\n",
        "    save_strategy='steps',\n",
        "    save_steps=110,\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=3e-5,\n",
        "    run_name=\"bart_asrs_finetune_v1\"\n",
        ")\n",
        "# Initialize model and trainer\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DivtwGjMNBR"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kPeJtlCMOhR"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "model.save_pretrained('./fine_tuned_bart')\n",
        "tokenizer.save_pretrained('./fine_tuned_bart')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72eToev6MRam"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Enable expandable segments to reduce memory fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Clear unused GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Verify GPU availability\n",
        "assert torch.cuda.is_available(), \"GPU is not available. Switch to a GPU runtime.\"\n",
        "\n",
        "# Ensure model is on GPU\n",
        "model = model.to('cuda')\n",
        "\n",
        "# Prepare test inputs\n",
        "test_inputs = tokenizer(test_df['Report 1_Narrative'].tolist(), max_length=1024, truncation=True, padding=True, return_tensors='pt')\n",
        "input_ids = test_inputs['input_ids'].to('cuda')\n",
        "attention_mask = test_inputs['attention_mask'].to('cuda')\n",
        "\n",
        "# Create a dataset and dataloader for batching\n",
        "dataset = TensorDataset(input_ids, attention_mask)\n",
        "batch_size = 4  # Define batch size\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Generate summaries in batches\n",
        "generated_summaries = []\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for batch in dataloader:\n",
        "        batch_input_ids, batch_attention_mask = batch\n",
        "        summary_ids = model.generate(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        batch_summaries = [tokenizer.decode(g, skip_special_tokens=True) for g in summary_ids]\n",
        "        generated_summaries.extend(batch_summaries)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "ground_truth = test_df['Report 1_Synopsis'].tolist()\n",
        "scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "for gen, ref in zip(generated_summaries, ground_truth):\n",
        "    score = scorer.score(ref, gen)\n",
        "    scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "    scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "    scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "avg_scores = {key: sum(values) / len(values) for key, values in scores.items()}\n",
        "print(\"Average ROUGE scores:\", avg_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hETytx4Odnnw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXQ96wkEMTOJ"
      },
      "outputs": [],
      "source": [
        "# Calculate exceeds_1024\n",
        "df['exceeds_1024'] = df['narrative_tokens'] > 1024\n",
        "\n",
        "# Group by primary_flight_phase\n",
        "stats_phase = df.groupby('primary_flight_phase')['narrative_tokens'].agg(\n",
        "    mean='mean',\n",
        "    median='median',\n",
        "    max='max',\n",
        "    count='count'\n",
        ").reset_index()\n",
        "\n",
        "stats_phase['proportion_exceeding'] = df.groupby('primary_flight_phase')['exceeds_1024'].mean().values\n",
        "\n",
        "print(\"Narrative Length Statistics by Flight Phase:\")\n",
        "print(stats_phase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bc4UjhXMnVt"
      },
      "outputs": [],
      "source": [
        "# Extract primary anomaly\n",
        "df['primary_anomaly'] = df['Events_Anomaly'].apply(lambda x: x.split(';')[0].strip())\n",
        "\n",
        "# Group by primary_anomaly\n",
        "stats_anomaly = df.groupby('primary_anomaly')['narrative_tokens'].agg(\n",
        "    mean='mean',\n",
        "    median='median',\n",
        "    max='max',\n",
        "    count='count'\n",
        ").reset_index()\n",
        "\n",
        "stats_anomaly['proportion_exceeding'] = df.groupby('primary_anomaly')['exceeds_1024'].mean().values\n",
        "\n",
        "print(\"Narrative Length Statistics by Primary Anomaly:\")\n",
        "print(stats_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAfvSGVRolMD"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Sentence-BERT model\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# After generating summaries\n",
        "# Print all BART-generated summaries with ground truth for inspection\n",
        "for i, (gen, ref, incident_id) in enumerate(zip(generated_summaries, ground_truth, test_df['_ACN'])):  # Include incident_id\n",
        "    # Calculate cosine similarity\n",
        "    gen_embedding = sbert_model.encode(gen, convert_to_tensor=True)\n",
        "    ref_embedding = sbert_model.encode(ref, convert_to_tensor=True)\n",
        "    cosine_sim = util.cos_sim(gen_embedding, ref_embedding).item()\n",
        "\n",
        "    print(f\"Sample {i+1} (Incident ID: {incident_id}):\")  # Print incident_id\n",
        "    print(f\"Ground-Truth Synopsis: {ref}\")\n",
        "    print(f\"BART-Generated Synopsis: {gen}\")\n",
        "    print(f\"Cosine Similarity: {cosine_sim:.3f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# ... (your existing code for loading models and generating summaries) ...\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Calculate and store ROUGE scores for all summaries\n",
        "all_rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "for gen, ref in zip(generated_summaries, ground_truth):\n",
        "    scores = scorer.score(ref, gen)\n",
        "    all_rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "    all_rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "    all_rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "# Calculate average ROUGE scores\n",
        "avg_rouge_scores = {key: sum(values) / len(values) for key, values in all_rouge_scores.items()}\n",
        "\n",
        "# Print average ROUGE scores\n",
        "print(\"Average ROUGE Scores for BART:\")\n",
        "for metric, score in avg_rouge_scores.items():\n",
        "    print(f\"{metric}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "Z4lyUlwKVwy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZZSILxXpb0h"
      },
      "outputs": [],
      "source": [
        "# After generating summaries and computing ROUGE scores\n",
        "# Display incident ID, narrative, ground-truth synopsis, and BART-generated synopsis for comparison\n",
        "for idx in range(len(test_df)):\n",
        "    print(f\"Sample {idx+1}:\")\n",
        "    print(f\"Incident ID: {test_df['_ACN'].iloc[idx]}\")\n",
        "    print(f\"Narrative: {test_df['Report 1_Narrative'].iloc[idx]}\")\n",
        "    print(f\"Ground-Truth Synopsis: {ground_truth[idx]}\")\n",
        "    print(f\"BART-Generated Synopsis: {generated_summaries[idx]}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jfG87vxp_vx"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Sentence-BERT model for cosine similarity\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Display incident ID, narrative, ground-truth synopsis, BART-generated synopsis, and cosine similarity\n",
        "for idx in range(len(test_df)):\n",
        "    # Compute cosine similarity\n",
        "    gen = generated_summaries[idx]\n",
        "    ref = ground_truth[idx]\n",
        "    gen_embedding = sbert_model.encode(gen, convert_to_tensor=True)\n",
        "    ref_embedding = sbert_model.encode(ref, convert_to_tensor=True)\n",
        "    cosine_sim = util.cos_sim(gen_embedding, ref_embedding).item()\n",
        "\n",
        "    print(f\"Sample {idx+1}:\")\n",
        "    print(f\"Incident ID: {test_df['_ACN'].iloc[idx]}\")\n",
        "    print(f\"Narrative: {test_df['Report 1_Narrative'].iloc[idx]}\")\n",
        "    print(f\"Ground-Truth Synopsis: {ground_truth[idx]}\")\n",
        "    print(f\"BART-Generated Synopsis: {gen}\")\n",
        "    print(f\"Cosine Similarity: {cosine_sim:.3f}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Sentence-BERT model\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Assuming you have generated_summaries and ground_truth lists\n",
        "cosine_similarities = []\n",
        "for gen, ref in zip(generated_summaries, ground_truth):\n",
        "    gen_embedding = sbert_model.encode(gen, convert_to_tensor=True)\n",
        "    ref_embedding = sbert_model.encode(ref, convert_to_tensor=True)\n",
        "    cosine_sim = util.cos_sim(gen_embedding, ref_embedding).item()\n",
        "    cosine_similarities.append(cosine_sim)\n",
        "\n",
        "# Calculate average cosine similarity\n",
        "avg_cosine_sim = sum(cosine_similarities) / len(cosine_similarities)\n",
        "print(f\"Average Cosine Similarity: {avg_cosine_sim:.3f}\")"
      ],
      "metadata": {
        "id": "G5UVQfkaDwfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load the saved test split\n",
        "test_dataset = load_dataset('csv', data_files='test_split.csv')['train']\n",
        "test_df = pd.DataFrame(test_dataset)\n",
        "\n",
        "# Load tokenizers and models\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('./results_t5')\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained('./results_t5')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('./fine_tuned_bart')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('./fine_tuned_bart')"
      ],
      "metadata": {
        "id": "H_7SUAGOkw1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK3_tKcAc9TQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWqBMnqolnGt"
      },
      "source": [
        "T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIQq5TykpZBZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9hVTED0piKe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypWDiRWqink9"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Text Normalization\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9.,!? ]', '', text)\n",
        "    return text\n",
        "\n",
        "df['Report 1_Narrative'] = df['Report 1_Narrative'].apply(normalize_text)\n",
        "df['Report 1_Synopsis'] = df['Report 1_Synopsis'].apply(normalize_text)\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files='cleaned_ASRS_data.csv')\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_val = dataset['train'].train_test_split(test_size=0.1111)  # To get 886 train, 110 val\n",
        "train_dataset = train_val['train']\n",
        "val_dataset = train_val['test']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + text for text in examples['Report 1_Narrative']]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(examples['Report 1_Synopsis'], max_length=150, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Fine-tune\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_t5',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_t5',\n",
        "    eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWT_WnX_x-vT"
      },
      "outputs": [],
      "source": [
        "generated_summaries_t5 = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for example in test_dataset:\n",
        "        input_ids = tokenizer(\"summarize: \" + example['Report 1_Narrative'], return_tensors='pt', max_length=512, truncation=True).input_ids.to('cuda')\n",
        "        outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_summaries_t5.append(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "nnR9krycmI8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the device is set correctly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to the correct device\n",
        "t5_model.to(device)\n",
        "bart_model.to(device)\n",
        "\n",
        "# Generate T5 summaries\n",
        "t5_inputs = [\"summarize: \" + text for text in test_dataset['Report 1_Narrative']]\n",
        "t5_input_encodings = t5_tokenizer(t5_inputs, max_length=256, truncation=True, padding=True, return_tensors='pt')\n",
        "t5_input_ids = t5_input_encodings['input_ids'].to(device)\n",
        "t5_attention_mask = t5_input_encodings['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    t5_summary_ids = t5_model.generate(\n",
        "        input_ids=t5_input_ids,\n",
        "        attention_mask=t5_attention_mask,\n",
        "        max_length=64,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "t5_summaries = [t5_tokenizer.decode(g, skip_special_tokens=True) for g in t5_summary_ids]\n",
        "\n",
        "# Generate BART summaries\n",
        "bart_inputs = bart_tokenizer(test_dataset['Report 1_Narrative'], max_length=256, truncation=True, padding=True, return_tensors='pt')\n",
        "bart_input_ids = bart_inputs['input_ids'].to(device)\n",
        "bart_attention_mask = bart_inputs['attention_mask'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    bart_summary_ids = bart_model.generate(\n",
        "        input_ids=bart_input_ids,\n",
        "        attention_mask=bart_attention_mask,\n",
        "        max_length=64,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "bart_summaries = [bart_tokenizer.decode(g, skip_special_tokens=True) for g in bart_summary_ids]\n",
        "\n",
        "# Compute ROUGE scores and Cosine Similarities\n",
        "ground_truth = test_dataset['Report 1_Synopsis']\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Lists to store scores\n",
        "bart_rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "t5_rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "bart_cosine_similarities = []\n",
        "t5_cosine_similarities = []\n",
        "\n",
        "for idx, (gt_synopsis, bart_summary, t5_summary, incident_id) in enumerate(zip(ground_truth, bart_summaries, t5_summaries, test_df['_ACN'])):\n",
        "    # ROUGE scores\n",
        "    bart_rouge = scorer.score(gt_synopsis, bart_summary)\n",
        "    t5_rouge = scorer.score(gt_synopsis, t5_summary)\n",
        "\n",
        "    bart_rouge_scores['rouge1'].append(bart_rouge['rouge1'].fmeasure)\n",
        "    bart_rouge_scores['rouge2'].append(bart_rouge['rouge2'].fmeasure)\n",
        "    bart_rouge_scores['rougeL'].append(bart_rouge['rougeL'].fmeasure)\n",
        "    t5_rouge_scores['rouge1'].append(t5_rouge['rouge1'].fmeasure)\n",
        "    t5_rouge_scores['rouge2'].append(t5_rouge['rouge2'].fmeasure)\n",
        "    t5_rouge_scores['rougeL'].append(t5_rouge['rougeL'].fmeasure)\n",
        "\n",
        "    # Cosine Similarities\n",
        "    bart_embedding = sbert_model.encode(bart_summary, convert_to_tensor=True)\n",
        "    t5_embedding = sbert_model.encode(t5_summary, convert_to_tensor=True)\n",
        "    gt_embedding = sbert_model.encode(gt_synopsis, convert_to_tensor=True)\n",
        "\n",
        "    bart_cos_sim = util.cos_sim(bart_embedding, gt_embedding).item()\n",
        "    t5_cos_sim = util.cos_sim(t5_embedding, gt_embedding).item()\n",
        "\n",
        "    bart_cosine_similarities.append(bart_cos_sim)\n",
        "    t5_cosine_similarities.append(t5_cos_sim)\n",
        "\n",
        "# Average scores\n",
        "avg_bart_rouge = {key: sum(values) / len(values) for key, values in bart_rouge_scores.items()}\n",
        "avg_t5_rouge = {key: sum(values) / len(values) for key, values in t5_rouge_scores.items()}\n",
        "avg_bart_cos_sim = sum(bart_cosine_similarities) / len(bart_cosine_similarities)\n",
        "avg_t5_cos_sim = sum(t5_cosine_similarities) / len(t5_cosine_similarities)\n",
        "\n",
        "print(\"BART Average ROUGE Scores:\", avg_bart_rouge)\n",
        "print(\"T5 Average ROUGE Scores:\", avg_t5_rouge)\n",
        "print(\"BART Average Cosine Similarity:\", avg_bart_cos_sim)\n",
        "print(\"T5 Average Cosine Similarity:\", avg_t5_cos_sim)\n",
        "\n",
        "# Save summaries with incident IDs for reference\n",
        "results_df = pd.DataFrame({\n",
        "    '_ACN': test_df['_ACN'],\n",
        "    'Ground_Truth_Synopsis': ground_truth,\n",
        "    'BART_Summary': bart_summaries,\n",
        "    'T5_Summary': t5_summaries\n",
        "})\n",
        "results_df.to_csv('summaries_with_incident_ids.csv', index=False)\n",
        "print(\"Summaries saved to 'summaries_with_incident_ids.csv'\")\n"
      ],
      "metadata": {
        "id": "TMa208RhlCWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few summaries for inspection, including incident number\n",
        "for i, (gen_summary, ref_summary, incident_id) in enumerate(zip(\n",
        "    generated_summaries_t5[:100],\n",
        "    test_dataset['Report 1_Synopsis'][:100],\n",
        "    test_dataset['_ACN'][:100]  # Include incident_id\n",
        ")):\n",
        "    print(f\"Sample {i+1} (Incident ID: {incident_id}):\")\n",
        "    print(f\"Ground-Truth Synopsis: {ref_summary}\")\n",
        "    print(f\"T5-Generated Synopsis: {gen_summary}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "uGo-LPK9fk2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "ground_truth = test_dataset['Report 1_Synopsis']\n",
        "for gen, ref in zip(generated_summaries_t5, ground_truth):\n",
        "    scores = scorer.score(ref, gen)\n",
        "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
        "avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
        "avg_rougel = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
        "print(f\"T5-Base ROUGE Scores: R-1: {avg_rouge1:.3f}, R-2: {avg_rouge2:.3f}, R-L: {avg_rougel:.3f}\")"
      ],
      "metadata": {
        "id": "FqinbIswT-BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UNEDipmyG5b"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "ground_truth = test_dataset['Report 1_Synopsis']\n",
        "for gen, ref in zip(generated_summaries_t5, ground_truth):\n",
        "    scores = scorer.score(ref, gen)\n",
        "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
        "avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
        "avg_rougel = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
        "print(f\"T5-Base ROUGE Scores: R-1: {avg_rouge1:.3f}, R-2: {avg_rouge2:.3f}, R-L: {avg_rougel:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "XdoY0AUUlxvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m6zNYgazaK2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set environment variable for memory management\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Text Normalization\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9.,!? ]', '', text)\n",
        "    return text\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files='cleaned_ASRS_data.csv')\n",
        "df = dataset['train'].to_pandas()\n",
        "df['Report 1_Narrative'] = df['Report 1_Narrative'].apply(normalize_text)\n",
        "df['Report 1_Synopsis'] = df['Report 1_Synopsis'].apply(normalize_text)\n",
        "\n",
        "# Save the normalized dataset back to a CSV and reload\n",
        "df.to_csv('normalized_ASRS_data.csv', index=False)\n",
        "dataset = load_dataset('csv', data_files='normalized_ASRS_data.csv')\n",
        "\n",
        "# Split dataset: 886 train, 110 val, 111 test\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)  # 111 test\n",
        "train_val = dataset['train'].train_test_split(test_size=0.1111)  # 886 train, 110 val\n",
        "train_dataset = train_val['train']\n",
        "val_dataset = train_val['test']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + text for text in examples['Report 1_Narrative']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')  # Reduced max_length\n",
        "    labels = tokenizer(examples['Report 1_Synopsis'], max_length=128, truncation=True, padding='max_length')  # Reduced max_length\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Fine-tune\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_t5',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    per_device_eval_batch_size=1,   # Reduced batch size\n",
        "    gradient_accumulation_steps=4,  # Simulate effective batch size of 4\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_t5',\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
        "    load_best_model_at_end=True,  # Load the best model at the end\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NWU86Ki8kuH"
      },
      "outputs": [],
      "source": [
        "# Explicitly save the model after training\n",
        "trainer.save_model('./results_t5')\n",
        "tokenizer.save_pretrained('./results_t5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds8WCCK38obN"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Set environment variable for memory management\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('./results_t5')\n",
        "tokenizer = T5Tokenizer.from_pretrained('./results_t5')\n",
        "model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "# Load the test dataset\n",
        "dataset = load_dataset('csv', data_files='cleaned_ASRS_data.csv')\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Prepare inputs for generation\n",
        "inputs = [\"summarize: \" + text for text in test_dataset['Report 1_Narrative']]\n",
        "input_encodings = tokenizer(inputs, max_length=512, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Create a dataset for batching\n",
        "class SummaryDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "summary_dataset = SummaryDataset(input_encodings)\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(summary_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Generate summaries in batches\n",
        "generated_summaries_t5 = []\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=150,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        summaries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "        generated_summaries_t5.extend(summaries)\n",
        "\n",
        "# Print a few summaries for inspection\n",
        "for i, (gen_summary, ref_summary) in enumerate(zip(generated_summaries_t5[:100], test_dataset['Report 1_Synopsis'][:5])):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Ground-Truth Synopsis: {ref_summary}\")\n",
        "    print(f\"T5-Generated Synopsis: {gen_summary}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few summaries for inspection\n",
        "for i, (gen_summary, ref_summary) in enumerate(zip(generated_summaries_t5[:], test_dataset['Report 1_Synopsis'][:30])):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Ground-Truth Synopsis: {ref_summary}\")\n",
        "    print(f\"T5-Generated Synopsis: {gen_summary}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "_fs4fTeT_a5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWaRBULV_JeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuQZzHNbEDfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Sentence-BERT model\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Assuming you have generated_summaries_t5 and ground_truth lists\n",
        "t5_cosine_similarities = []\n",
        "for gen_t5, ref in zip(generated_summaries_t5, ground_truth):\n",
        "    gen_t5_embedding = sbert_model.encode(gen_t5, convert_to_tensor=True)\n",
        "    ref_embedding = sbert_model.encode(ref, convert_to_tensor=True)\n",
        "    cosine_sim_t5 = util.cos_sim(gen_t5_embedding, ref_embedding).item()\n",
        "    t5_cosine_similarities.append(cosine_sim_t5)\n",
        "\n",
        "# Calculate average cosine similarity for T5\n",
        "avg_cosine_sim_t5 = sum(t5_cosine_similarities) / len(t5_cosine_similarities)\n",
        "print(f\"Average T5 Cosine Similarity: {avg_cosine_sim_t5:.3f}\")"
      ],
      "metadata": {
        "id": "Q8LPtYdhEVUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "E457XzEPiIgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}